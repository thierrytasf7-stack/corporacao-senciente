# Guia de Instala√ß√£o e Configura√ß√£o do Ollama

## üéØ Por que usar Ollama?

Ollama permite executar modelos LLM **localmente**, sem rate limits e sem custos por requisi√ß√£o. √â ideal para:
- **Treinamento massivo** de agentes
- **Gera√ß√£o de exemplos sint√©ticos** em grande volume
- **Evolu√ß√£o de prompts** com muitas itera√ß√µes
- **An√°lise competitiva** extensiva

### Vantagens:
- ‚úÖ **Sem rate limits** - Execute quantas requisi√ß√µes precisar
- ‚úÖ **Sem custos** - Gratuito ap√≥s instala√ß√£o
- ‚úÖ **Privacidade** - Dados n√£o saem da sua m√°quina
- ‚úÖ **Velocidade** - Sem lat√™ncia de rede (ap√≥s primeira carga)
- ‚úÖ **Offline** - Funciona sem internet

### Desvantagens:
- ‚ö†Ô∏è Requer GPU/CPU potente (recomendado: 8GB+ RAM, GPU opcional)
- ‚ö†Ô∏è Primeira execu√ß√£o baixa o modelo (pode ser grande)
- ‚ö†Ô∏è Qualidade pode ser menor que Grok/Gemini (depende do modelo)

---

## üì• Instala√ß√£o

### Windows

1. **Baixar Ollama:**
   - Acesse: https://ollama.com/download
   - Baixe o instalador para Windows
   - Execute e instale

2. **Verificar instala√ß√£o:**
   ```powershell
   ollama --version
   ```

3. **Baixar um modelo:**
   ```powershell
   # Modelo recomendado para treinamento (equil√≠brio qualidade/velocidade)
   ollama pull llama3.2
   
   # Ou modelos maiores (melhor qualidade, mais lento)
   ollama pull llama3.1:8b
   ollama pull mistral
   ollama pull qwen2.5
   ```

4. **Testar:**
   ```powershell
   ollama run llama3.2 "Ol√°, como voc√™ est√°?"
   ```

### Linux/Mac

```bash
# Instalar via script oficial
curl -fsSL https://ollama.com/install.sh | sh

# Baixar modelo
ollama pull llama3.2

# Testar
ollama run llama3.2 "Hello!"
```

---

## ‚öôÔ∏è Configura√ß√£o no Projeto

### 1. Atualizar `env.local`

Adicione as seguintes vari√°veis:

```env
# Ollama (Modelo Local)
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# Configura√ß√£o de Fallback
USE_LOCAL_FOR_TRAINING=true  # Usar Ollama automaticamente para treinamento
```

### 2. Modelos Recomendados

| Modelo | Tamanho | RAM M√≠nima | Qualidade | Velocidade | Uso |
|--------|---------|------------|-----------|------------|-----|
| `llama3.2` | 3B | 4GB | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö°‚ö° | Treinamento r√°pido |
| `llama3.1:8b` | 8B | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | Treinamento balanceado |
| `mistral` | 7B | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | Qualidade/velocidade |
| `qwen2.5:7b` | 7B | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° | Multil√≠ngue |
| `llama3.1:70b` | 70B | 48GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° | M√°xima qualidade |

**Recomenda√ß√£o inicial:** `llama3.2` para come√ßar r√°pido, depois migrar para `llama3.1:8b` se tiver RAM suficiente.

### 3. Verificar se est√° funcionando

Execute:

```bash
node -e "
import('./scripts/utils/llm_client.js').then(async ({ checkOllamaAvailable }) => {
  const available = await checkOllamaAvailable();
  console.log(available ? '‚úÖ Ollama est√° dispon√≠vel' : '‚ùå Ollama n√£o est√° dispon√≠vel');
});
"
```

---

## üöÄ Uso Autom√°tico

O sistema **automaticamente** usa Ollama quando:

1. **Treinamento** (`isTraining: true`):
   - Gera√ß√£o de exemplos sint√©ticos
   - Evolu√ß√£o de prompts
   - An√°lise competitiva

2. **Rate limit do Grok**:
   - Se Grok retornar 429, o sistema tenta Ollama automaticamente

3. **Fallback final**:
   - Se Grok e Gemini falharem, tenta Ollama como √∫ltimo recurso

---

## üîß Troubleshooting

### Ollama n√£o est√° respondendo

```powershell
# Verificar se est√° rodando
ollama list

# Reiniciar servi√ßo (Windows)
# Abra o Task Manager e finalize "Ollama", depois reinicie o app

# Verificar porta
netstat -an | findstr 11434
```

### Modelo n√£o encontrado

```powershell
# Listar modelos instalados
ollama list

# Baixar modelo novamente
ollama pull llama3.2
```

### Muito lento

- Use modelo menor (`llama3.2` ao inv√©s de `llama3.1:8b`)
- Feche outros aplicativos pesados
- Considere usar GPU (se dispon√≠vel)

### Erro de mem√≥ria

- Use modelo menor
- Feche outros aplicativos
- Considere aumentar RAM ou usar modelo quantizado

---

## üìä Compara√ß√£o de Performance

### Grok (Cloud)
- ‚úÖ Melhor qualidade
- ‚úÖ Muito r√°pido
- ‚ùå Rate limits
- ‚ùå Custo por requisi√ß√£o

### Ollama (Local)
- ‚úÖ Sem rate limits
- ‚úÖ Gratuito
- ‚úÖ Privacidade
- ‚ö†Ô∏è Qualidade menor (depende do modelo)
- ‚ö†Ô∏è Pode ser mais lento (sem GPU)

### Gemini (Cloud)
- ‚úÖ Boa qualidade
- ‚úÖ Generoso rate limit
- ‚ö†Ô∏è Pode ter limites em uso intenso

---

## üéØ Estrat√©gia Recomendada

1. **Treinamento massivo**: Use Ollama (`USE_LOCAL_FOR_TRAINING=true`)
2. **Produ√ß√£o/Decis√µes cr√≠ticas**: Use Grok (melhor qualidade)
3. **Fallback autom√°tico**: Sistema tenta Grok ‚Üí Ollama ‚Üí Gemini ‚Üí Together

---

## üìö Recursos

- **Documenta√ß√£o oficial**: https://ollama.com/docs
- **Modelos dispon√≠veis**: https://ollama.com/library
- **API Reference**: https://github.com/ollama/ollama/blob/main/docs/api.md

---

## ‚úÖ Checklist de Setup

- [ ] Ollama instalado
- [ ] Modelo baixado (`ollama pull llama3.2`)
- [ ] Teste manual funcionando (`ollama run llama3.2 "test"`)
- [ ] Vari√°veis adicionadas ao `env.local`
- [ ] `OLLAMA_ENABLED=true`
- [ ] `USE_LOCAL_FOR_TRAINING=true`
- [ ] Teste de integra√ß√£o passou

---

## üîÑ Pr√≥ximos Passos

Ap√≥s configurar Ollama:

1. Execute o treinamento novamente:
   ```bash
   node scripts/cerebro/self_improvement_orchestrator.js --phase=synthetic --agents=copywriting
   ```

2. Monitore o uso:
   - Ollama ser√° usado automaticamente para treinamento
   - Grok ser√° usado para produ√ß√£o (se dispon√≠vel)

3. Ajuste conforme necess√°rio:
   - Mude `OLLAMA_MODEL` se quiser outro modelo
   - Ajuste `USE_LOCAL_FOR_TRAINING` se preferir sempre Grok























