# OtimizaÃ§Ãµes de Performance Implementadas

## ðŸš€ Melhorias na Chamada do Ollama

### 1. **LimitaÃ§Ã£o de Tokens**
- `num_predict: 800-1000` tokens por resposta
- Respostas mais curtas = processamento mais rÃ¡pido
- Ideal para treinamento onde precisamos de mÃºltiplas chamadas

### 2. **Timeout Otimizado**
- 25-30 segundos por chamada
- Evita travamentos em respostas muito longas
- Fallback automÃ¡tico se timeout

### 3. **Contexto Reduzido**
- `num_ctx: 2048` (ao invÃ©s de padrÃ£o 4096)
- Menos memÃ³ria = mais rÃ¡pido
- Suficiente para prompts de treinamento

## ðŸ“¦ Processamento em Batches

### ConfiguraÃ§Ãµes de Batch

| Tipo | Batch Size | Delay | Concurrent |
|------|------------|-------|------------|
| **Synthetic** | 3 itens | 1s | 1 (sequencial) |
| **Research** | 5 itens | 0.5s | 2 |
| **Prompts** | 1 item | 2s | 1 |
| **Vectorization** | 10 chunks | 0.5s | 1 |

### BenefÃ­cios

1. **Progresso VisÃ­vel**: VocÃª vÃª o progresso em tempo real
2. **Menos Erros**: Se um batch falhar, os outros continuam
3. **Sem Travamentos**: Processamento pequeno = menos chance de travar
4. **RecuperaÃ§Ã£o**: Sistema continua mesmo se alguns itens falharem

## ðŸ”„ Sistema de Tasks Progressivas

### Funcionalidades

1. **CriaÃ§Ã£o de Tasks**: Cada processo cria uma task no banco
2. **AtualizaÃ§Ã£o de Progresso**: Progresso salvo em tempo real
3. **Retry Inteligente**: Retry automÃ¡tico com backoff exponencial
4. **Timeout por Item**: Cada item tem timeout individual

### Exemplo de Uso

```javascript
// Criar task
const task = await createProgressTask('qa_generation', 'copywriting', 10);

// Processar em batches com progresso
const results = await processInBatches(
    items,
    processor,
    'synthetic',
    {
        onProgress: (progress) => {
            updateTaskProgress(task.id, progress.processed, progress);
        }
    }
);

// Finalizar
await completeTask(task.id, true, results);
```

## âš¡ OtimizaÃ§Ãµes EspecÃ­ficas

### GeraÃ§Ã£o de Exemplos SintÃ©ticos

**Antes:**
- Gerava 10 Q&A de uma vez
- 1 chamada LLM grande
- Risco de timeout/travamento

**Depois:**
- Gera 1 Q&A por vez
- Batches de 3
- Timeout de 25s por item
- Progresso visÃ­vel

### Valores PadrÃ£o Reduzidos

- **Q&A**: 6 (ao invÃ©s de 10)
- **Failure Cases**: 3 (ao invÃ©s de 5)
- **Success Patterns**: 3 (ao invÃ©s de 5)

**Motivo**: Qualidade > Quantidade. Menos exemplos bem gerados Ã© melhor que muitos ruins.

## ðŸ“Š Monitoramento

### Logs de Progresso

```
[INFO] Processando batch 1/3 (3 itens, progress: 0/9)
[INFO] Progresso Q&A: 33%
[INFO] Processando batch 2/3 (3 itens, progress: 3/9)
[INFO] Progresso Q&A: 66%
```

### Tasks no Banco

- Status: `in_progress`, `completed`, `failed`
- Progresso: `processed_items`, `progress_percentage`
- Metadata: InformaÃ§Ãµes detalhadas do processo

## ðŸŽ¯ Resultados Esperados

1. **Velocidade**: 3-5x mais rÃ¡pido (batches pequenos)
2. **Confiabilidade**: 90%+ de sucesso (retry + timeout)
3. **Visibilidade**: Progresso em tempo real
4. **ResiliÃªncia**: Continua mesmo com falhas parciais

## ðŸ”§ ConfiguraÃ§Ã£o

### Ajustar Batch Size

Edite `scripts/cerebro/task_scheduler.js`:

```javascript
const BATCH_CONFIG = {
    synthetic: {
        batchSize: 3, // Aumentar para mais velocidade (mais risco)
        delayBetweenBatches: 1000, // Reduzir para mais velocidade
    },
};
```

### Ajustar Timeout

Edite chamadas LLM:

```javascript
await callLLM(prompt, systemPrompt, 0.7, {
    isTraining: true,
    maxTokens: 600, // Reduzir = mais rÃ¡pido
    timeout: 20000, // Reduzir = mais rÃ¡pido (mais risco)
});
```

## âœ… Checklist de OtimizaÃ§Ã£o

- [x] LimitaÃ§Ã£o de tokens no Ollama
- [x] Timeout otimizado
- [x] Processamento em batches
- [x] Sistema de tasks progressivas
- [x] Retry inteligente
- [x] Logs de progresso
- [x] Valores padrÃ£o reduzidos
- [x] Delays entre batches

---

**Resultado**: Sistema mais rÃ¡pido, confiÃ¡vel e visÃ­vel! ðŸš€























